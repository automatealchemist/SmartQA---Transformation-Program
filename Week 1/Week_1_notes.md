# Introduction (Preamble): 

 - Limited View of Testing: We usually consider testing as an act of doing writing, executing test cases, etc., but that is a limited view, testing is not just about doing these tasks but also about questioning 
 -Incorrect Dichotomy: Manual and automation testing are incorrect dichotomy as automation testing is considered to be aid of manual testing, also some something is either manual task or automation task,
 - Incomplete View of Testing: Considering testing as the bounded task of possibly compliance / checking however, in real testing, it should not be bounded to these tasks Instead, testing should be an act of exploring the software, discovering something that has not been discovered yet, and finding something new 
 - Limited Approach to Testing: Approach can be limited as people rely too much on documentation however, in reality, there should be a balance between execution and documentation 
 - False Trust: People also think that automated testing may be superior to manual testing however, that's not the case
 - Myth of Testing: Another thought is that testing is driven by experience. However, testing is not totally dependent on experience, having experience adds value to testing depending on how much we have learn from our mistakes. Experience is polished by discipline. When we do something with discipline, we gain experience we become good, but not just by experience but also with a certain amount of formalism and ability to apply the formalism, which is ultimately embellished by experience.


# The TRANSFORMATION FOCUS 

 Doing vs. Probing: Instead of seeing testing as an act of doing, we should focus on seeing testing as an act of constantly probing intelligence.
 - Process vs Practice - Again, when it comes to testing, instead of just focusing of the process of doing the testing, we should also see it as the practice of testing, i.e. how we do.
 - Machine vs Intellect - Instead of using just machine or automation as an aid for testing, also use intellect alongside it as intellect is a serious and critical component that cannot be ignored.
 - Check (comply) vs Test (discover) - The Notion of compliance is checking and testing, which is actually discovering new things in the true sense.
 - Experience vs Method - Relying not only on experience but also on the method that allows us to think and do in addition to experience.
 - Detection vs preemption - It's not only about detection but also preemption and prevention.
 - Doing more vs To-Do Less - Instead of doing more, our focus should be on doing less and thinking more.

When we pick a component on right side on comparison points of Transformation focus then the smart testing begins i.e the notion of doing less and accomplishing more with meaningful thinking, appropriate tooling and well formed practices in addition to mature processes. This makes the assurance not only smarter but also meaningful.
 - 

>Where the shift will be : 
 - From an act of doing, we have to shift our focus to the act of probing continuously.
 - Along with that,t our next shift will be from doing more to doing less. 
 - Lastly, we have to shift our focus from the role of doer of testing activity to being a value deliverer through testing.


***Testing is deep probing, not mere code validation, and  requires smart thinking along with great habits.***


>For smart quality assurance, a deep probing mindset and building habits is required. 

| Typical Act that we need to do | Personal Transformation  | Enhanced | Outcomes  |
|--------------------------------|-------------------------|-----------|-----------|
| Do diligently                 | Of mindset             | Probe intelligently | Do effectively > Think well |
| Process Based                 | Of Activities          | Superior Practice | Do Efficiently |
| Experience Based              | Of Outcomes            | Method Based | Do Less & accomplish more |
| Reliance on Machine           | Of Roles & Values      | Intellect driven | -- |
| Detect                        | At Individual level    | Preempt | -- |
| Do More probing            | At Team Level       |  Do Less | -- |


>Smart testing is an intellectual practice of probing deeper to seek clarity and in the process, uncover and preempt issues rapidly, not limited to mere validation of code.

> Our agenda should be mentally active and physically lazy to do smart testing and achieve more. We should focus on doing the least amount of work and achieving the maximum amount of result.


Principles & Suggestions For Smart Quality Assurance: 
1. Time - While looking at time, we think of how we can do it faster.
2. Cost - The Same goes for cost that, how can we achieve being cheaper?
3. Quality - How can we achieve and deliver good quality?

Balancing the above 3 points is difficult but important for smart quality assurance.
Since balancing this traid is difficult, we instead focus on : 

 - I can do what I do well  
 - I can do what I do quickly 
 -  I can do what ido early 
 - I can do what I do less 
 - I should avoid doing unnecessary work. if there is something I can avoid, I should completely avoid it.
   

Some other suggestions for : 
1. Prioritize on focusing on where we are going rather than how fast we are going. As direction is more important than speed initially.
2. Focus on practice & process than pick tools. Understanding the best practices and processes of the organization where we work and then picking the tool accordingly will help us in achieving smart testing. 
3. Before picking automation scripts, focus on scenarios that need to be covered and why they need to be covered. 
4. Even though the qa documentation is important still, focus on writing less and doing more. 
5. Focus on test potency, not quantity, i.e., write effective test cases that bring better test results instead of writing the number of irrelevant test cases.
6. Again, instead of focusing on the number of issues, let's focus on the quality of the issues. 
7. Focus on the ket measure - "Escapes," i.e, the defect we missed, as they tell us a lot about our testing.
8. When it comes to defects we missed, it's not all about RCA, it's about our learning about the type of the defect and other details.
9. Using artificial intelligence as a tool for assistance for human intelligence 

>Testing is not about product validation, but it is about adding value to the context value to the organization value to the customer and ultimately value to the community.
>Our testing should be more focused on smart testing without disrupting the rhythm of the development.

***Testing is probing for issues pre-code and post-code. Probing can be at intellectual as well as physical level. Probing can be experiments, exploratory, etc., but probing cannot be just limited to code.***

 HyBIST = Hypothesis-based immersive session testing 
 which is the combination of hypothesis basis and immersion, which helps us in intelligent probing and observing.

Let's take an example - suppose we are suffering from abdominal pain and we approach 3 doctors. Then each of those 3 doctors may have different approaches for diagnosing our pain: 

1.  **One person prefers an invasive approach** – They believe that to get the best results, they need to perform surgery (cut open the abdomen), even though it is difficult, costly, and involves more risk.
    
2.  **Another person prefers a logical, step-by-step approach** Instead of immediately jumping to surgery, they ask many questions, analyze symptoms, and use logical deduction to diagnose the issue. Once they have a strong suspicion, they confirm it with specific tests.
    
3.  **A third person prefers a minimally invasive approach** Instead of fully opening the body, they suggest using laparoscopy (a small camera inserted through a tiny cut) to examine the problem with minimal impact.

- Being testers, we don't have infinite time for testing, so when it comes to activity, then while doing probing, we have to do it smartly. 
- Another is the act of design, which is thinking and designing a smart idea.

HyBIST = Hypothesis basis+Immersive session. This hypothesis-based approach helps in coming up with an intelligent or smart approach or probes aided by key concepts(12), and the immersive session type testing approach helps in probing smartly enabled by key practices (5). 

 - The act of doing should be efficient, optimal, very mindful, and immersive.
 - Use tools wherever is possible for bringing better results, use those
   tools.
   
>We currently have 12 key concepts and 5 key practices, and the key concepts can be used at different levels of testing the  life-cycle, while key-practices are applicable across all parts of the life-cycle. 

We should choose to do less and accomplish more.
At the same time, it should be human-powered and machine-aided.
Also, we should focus on detecting the issues well and strive to prevent.
>Let's be lean and stay agile. The ultimate goal should be to do great work and deliver value.

## Testing vs Checking

During the waterfall days, testing was viewed as an act of checking for compliance. 

But with time, the definition of testing also evolved, and now it is more considered as exploration and discovery.

Great quality is about ensuring compliance and discovering potential gaps.
Checking is more of like we know something and we are ensuring that the software does not misbehave for that expecations.

We need to be more mindful while doing testing so that we can ensure that the testing done in the present is robust and, hence, the future of the software is secure and we don't need to be worried about the future bugs that may occur.


Checking is a well-formed comparison win in which tools can do as we know what to do and what we expect, and it can be automated. It can be based on specs and is more about wellness, while testing is more about illness in which we intentionally inject something and check the behavior to make the software resistant for the future.


| Checking                     | Testing                  |
|-----------------------------|--------------------------|
| is comparing                | is questioning           |
| can be scripted             | not always scripted      |
| binary outcome Pass/fail    | Outcome Pass/fail        |
| design approach - logical, experience | design approach - many |
| based on spec               | beyond spec              |
| wellness                    | illness                  |

>Testing is a brilliant combination of checking for expected outcomes, exploring the whole, looking for the unexpected, uncovering issues, suggesting needs not yet thought of, improving what is done, and sensitizing to prevent issues.
"Testing" really is a gateway to doing good work and producing systems of value.

*We are often evaluated through tests, especially in school and college, using numbers like marks or percentages. These scores may suggest how intelligent or successful someone is, but they are not complete indicators of a person's true abilities.
While these evaluations give us a sense of where we stand, they are limited. Real understanding or performance can come from multiple perspectives like logic, user perspective, environment, personality, and so on. Looking at things from many angles offers a more complete picture than just relying on one number or metric.
Even if we use numbers like 80% to represent coverage or performance, they are just indicators not absolute truths. As we grow older, we begin to realize that life is not about fixed answers or rankings. Everything is more relative and layered than it seems at first glance.*

>Let's focus on the big-picture view of _testing vs checking_, focusing on the **objective behind the activity**, not just the action itself.

They highlight the **difference in roles** between early-stage and late-stage testing—what _I_ do versus what _others_ might do. The scope of what is being tested also varies: it could be something small like a feature, screen, API, or component, or something broader like a complete flow or requirement.

The speaker encourages looking at testing from **multiple points of view**—from a system's perspective, from the end user's angle, and considering the environment as well. They also touch upon **different testing states**—like new features, modified parts, enhancements, and fixes.

At the heart of it all is the **notion of value**. Testing is not just about finding bugs—it is about understanding the objective, knowing your role, and ensuring that value is delivered.


|Testing  |Checking  |Objective  |
|Dev Test(UT,IT ) |QA Test(System Test)  |Role  |
| in the small |in the large  |Entity  |
|Functional Test |Non-Functional Test  |What to Find  |
|By Human |Automated  |How To Find  |
|System POV	 |User POV  |POV  |
|New | Modified |State  |
|Detect | Prevent (Shift-left)  |Value  |

We can refer ourselves as a “TV QA tester” — a playful nod to how roles are sometimes labeled or perceived.

As we see in many organizations, a QA tester is often seen as someone who **designs test cases**, **defines testing approaches**, and performs **execution and evaluation**. It is a mix of **design thinking**, **hands-on activity**, and **emotional involvement**, not just mechanical checking.

The managers or team leads in mind, should be encouraged not to confine testers strictly to roles like designer, automation engineer, or executor. Instead, they urge leaders to give testers the **freedom to explore**, **contribute meaningfully**, and **add value** beyond fixed job titles.

There is significant importance of involving testers **early and late in the development lifecycle**, engaging with analysis, offering suggestions, and maintaining connections with support teams and others—especially in product companies.

Ultimately, the true measure of a tester’s value lies in whether their work positively impacts the **customer’s customer**. The speaker introduces the idea of viewing the QA role as a "smart assurance" role—broad, strategic, and deeply impactful.

They then transition into outlining specific concepts for upcoming sessions. One of the key ideas is around **entity granularity**—shifting the focus of testing from general phrases like "I am testing a feature or use case" to identifying what exactly is being tested at a fundamental level, such as a **structural compound** or basic building block.

The granularity of Entity Under Test(EUT)
When we test UI, API, library, etc. we are actually testing structural component, it is just small component which is not deliverable. But as soon as we start testing something that is deliverable it is called feature e.g. function with two buttons, etc. something with which we can do something. 
So when we comes to testing we need to have clarity on what exactly we are testing : 
1. structural component - basic building block 
2. techincal feature - basic offering from system 
3. user requirement - enables an user to do task 
4. business flow - a set of tasks by differnet users to accomplish a business objective

When we break down what we are testing, it helps to think in layers of clarity. At the most basic level, we might be testing a **building block**—something like a UI element, a function, a class, an object, a small service, or a library. These pieces on their own do not deliver value; they are just structural compounds.

As these building blocks come together and start offering a usable action—like a function combined with a couple of UI elements—they become a **feature**. This is something that a user can interact with and perform an action on.

If that feature is tied directly to a **user need** or use case, then we are testing a **requirement**—something that fulfills a specific user expectation.

And when multiple requirements or features are strung together to complete a meaningful task—like editing a document, running a spell check, and emailing it—that becomes a **business flow**.

So, when we ask, "What exactly am I testing?" it helps to clarify whether we are working with a building block, a feature, a requirement, or a full flow. This kind of granularity sharpens our understanding and testing strategy.

When we say business flow it means testing a set of tasks by different users that helps in achieveing business goals. 

If we got lost in the early stage we miss the top , if we focus on top we miss the bottom so we need to have telescope and microscope at the same time.
It will be difficult to have both, so while testing we need to have clarity on what exactly we are testing. Business flow, or user requirement or technical feature or structural component. And hence our everything like strategy, automation script should be well aligned accordingly.

*We should not try to do everything with a single test case. For example, trying to catch a spelling mistake _and_ check an entire business flow in the same test case will make things unnecessarily complex—and difficult to automate. It is better to keep things focused and well-scoped.
The first core idea is to be **clear about the entity** we are testing. When we receive a user story, we often assume it reflects a _requirement_. But in reality, many user stories feel more like upgraded features rather than clear requirements. That makes us pause and think—_Is this really my responsibility to test, or is something missing here?_
In the bigger picture, every application is made up of **entities of different sizes and shapes**—some are building blocks, some are features, some are full requirements, and others are end-to-end flows. Understanding the granularity of what we are testing is key to designing better, more meaningful tests.*

It is not just about the role we are assigned—it is about the **limited time** we have to test something effectively. If we are working within a fast-paced sprint and someone hands us a feature to validate, we need to be very clear about our **focus area**. Without that clarity, we risk spending our time on things that are not useful or relevant, which leads to inefficiency.

For instance, if we start testing from the very bottom (like low-level components) when our responsibility is at the system level, we might never reach the top-level scenarios in time. That is why we must **clearly define boundaries**—what is in scope for us and what is not. Knowing what _not_ to test is just as important as knowing what to test.

User stories should be like requirement, but mostly it looks like an upgraded feature. 
We get a fetaure and limited amount of time to test so if we don't have clarity on what we are testing then we may miss the picture as well as the time.

So we need to ensure that there are certain things that are not our responsibility which we need to do. Setting boundaries is important. We need to know what's our role. 

In a fast-paced sprint, our challenge is **not just about roles or formal responsibilities**, but about **working wisely within limited time**. When we are asked to validate something, we must have a **clear mental model** of where our focus lies. Without this clarity, we risk spending effort on areas that are either **not our responsibility** or **not the best use of time**, leading to inefficiency.

If we begin testing at the lowest level (like unit or integration) during a system-level test phase, we may never reach the broader flow-level validations in time. That is why setting **realistic and meaningful boundaries** is essential. We must know what **not to test**, and it should be clearly communicated—_this part is not mine, someone else is responsible for that._

When boundaries are not respected or when assumptions are made like "you are the tester, so clean everything," we end up in flawed practices. And if defects are found later, especially those that should have been caught earlier, **blame starts circulating unnecessarily**. The idea of "escapes" (defects found after release) needs to be **understood in context**—not all escaped defects are the tester's fault.

Ultimately, while quality is everyone’s responsibility, in practice, **responsibility needs structure**. Without clarity on what entities we are testing (UI, features, flows), or the kinds of defects we are supposed to find (UI bugs, integration issues, requirement mismatches), the system remains chaotic.

To solve this, we must introduce structured thinking:

-   **Entity granularities** – What exactly are we testing?
    
-   **Defect types** – What kinds of issues are we expected to catch?
    
-   **Quality levels** – At what level is this testing happening?
    

When these are understood, everything starts falling into place. Otherwise, it becomes a blame game dressed in the name of "shared responsibility."

A major reason why defects slip through and testers get blamed later is because **many testers lack clarity on where to draw the line**. They often do not consciously identify _what type of entity they are testing_—whether it is a component, a feature, or a full business flow. This lack of structured thinking leads to **scattered and unfocused testing**.

Most testers, even with around five years of experience, tend to mix different layers in their test cases. For example, when asked to test a business flow, their test cases might cover UI elements, back-end logic, and system components all together—without clearly anchoring to one level of testing. This ends up creating confusion and inefficiencies.

What is really missing is a **formal, method-based approach to defining test scope**. The issue is not about manual versus automation—it is about not being taught or trained to identify and limit testing to specific granular entities. Over 85% of testers might be missing this understanding.

This is not a criticism of testers—it is a gap in **education and systemic thinking**. If testers are guided to align their test design with the appropriate entity level (component, feature, requirement, business flow), the process becomes much more robust and predictable. Until then, many test cases will remain a mix of everything, and testing outcomes will remain unpredictable.

*When we talk about quality, it is not just about checking things during validation. Quality exists at every stage. There is quality in the way a requirement is written, quality in how the code is implemented, and also quality in how it is tested. If all of these are handled properly, things work well. The product becomes stable and easy to work with.
What stands out is that quality is not just one person's responsibility. Whether someone is a developer, a QA, or a product owner, everyone plays a part. If a bug comes up because the requirement was not clear, it is not fair to simply say it is a developer’s fault. It started with a gap in understanding.
So instead of pointing fingers, it helps if we understand what kind of issue it is. This makes it easier to group the issues properly. That way, we can work on fixing the actual problem instead of blaming each other.
When we look at bugs with this mindset, we focus more on how to avoid similar problems next time. And that makes our work stronger as a team.*


**Fractional Distillation in S/W Testing**

In school, we learned about different ways to separate mixtures in chemistry. There is filtration, evaporation, distillation, and fractional distillation. For example, if you mix sand and water, you can use a strainer. If it is salt and water, you let the water evaporate. And if it is oil, diesel, and petrol, you use fractional distillation by heating the mixture. Each part separates based on its boiling point.

This simple idea inspired a deep thought about testing.

*Think of any system under test as a mixture of bugs. These bugs are mixed in such a way that it is hard to spot them easily. Our job as testers is to separate and uncover them in a clean and efficient manner.*

Now imagine if we had a tool like a fractional distillation column in testing. It would be great if business-level issues rise to the top and development-level or structural bugs stay at the bottom. This would help us focus on the right things at the right time.

But in reality, leaders and managers are not always interested in how we do it. What they care about is the outcome. They want us to deliver value with less effort. Their focus is on results. They want fewer bugs in production, less support, and more customer satisfaction. If that happens, then they feel the work was done well.

If the opposite happens, they will not say it directly, but it means something went wrong. No one will say the process is terrible, but the result speaks for itself. So we should not expect people to like us just because we followed a good method. What matters is the outcome.

Now coming back to testing.

Think of the system as having different types of issues. There are basic UI problems, behavior issues, attribute issues, and even deployment problems. If we divide them into different levels, we can design focused tests to find each type. This helps us reduce confusion and improves our chances of finding issues in a smarter way.

This idea follows a simple principle. *Divide the problem and focus on one part at a time. It does not mean we will find everything in one go. Some issues will show up later. Some might be found by someone else. And that is fine.*

Sometimes we work as a developer in the morning and as a tester in the afternoon. It is not about job titles. It is about the role we play and the mindset we carry. Once we understand our role well, we naturally become more careful. We try to prevent issues before they happen.

And that is what we should aim for. That is the kind of awareness and discipline that makes us better at what we do.



In the earlier parts, we talked about different types of entities and the idea of granularity. Then we explored the metaphor of distillation to separate different types of issues. Building on that, let us now shift our focus to the concept of _levels_.

You might ask, why nine levels? The number is not that important. It could have been ten or eleven. But over many years of thinking and simplifying, nine just felt right. What matters more than the number is the idea itself—the idea that quality can be broken down into layers, into well-structured levels that make it easier for us to find, fix, and improve.

So here is the thought: what if we looked at software quality as a staircase, where each step takes you closer to a better product?

Le's explore each of these levels.

**Level 1: Input Quality**  
It all starts with inputs. If you give poor inputs, then everything that follows is likely to go wrong. You might remember this as "defensive programming" from the old days. The idea is simple: validate what comes in.

**Level 2: Interface Quality**  
Even if the inputs are good, the way they are passed matters. Say, I send the date and the month, but in the wrong order. Each value is fine, but the arrangement is incorrect. That is an interface-level issue.

**Level 3: Structural Quality**  
Now imagine the data is passed correctly, but internally, the structure is flawed. Maybe one thread reads the date and another reads the month, and due to poor handling, they go out of sync. This is not a behavioral issue—it is structural.

**Level 4: Behavioral Quality (Individual)**  
The structure is fine, but the system behaves incorrectly. It gives the wrong output. This is where the core logic is broken, even if the data flow looks clean.

**Level 5: Behavioral Quality (Composite)**  
Each part behaves well on its own, but when combined—say, in a travel app where discounts are calculated for senior citizens—it fails. This is an issue with composite behavior.

**Level 6: Environment Compatibility**  
Everything works well in your machine. But once you run it on another operating system or different hardware, something breaks. The behavior changes due to the environment.

**Level 7: Attribute-Level Quality**  
Now it works across environments, but is it fast enough? Is it secure? Is it reliable? Here we test non-functional aspects—performance, security, and more. These are attributes that shape the user's experience.

**Level 8: Real-World Deployment**  
The system runs fine in the lab. But once it is deployed with real customer data, things fall apart. Data might not migrate properly, or edge cases show up in live use. This is where the rubber meets the road.

**Level 9: Business Value**  
And finally, the most important one—the customer says, "Yes, this software is adding value to my business." It helps them save time, reduce costs, or grow their operations. This is the true success—the impact it makes in the real world.

----------

All these levels are not just boxes to check. They are perspectives—ways of seeing the product in layers. Most of these levels are under our control as engineers and testers. And if we get them right, we reach that final level where the customer says: _this software is not just working, it is working **for me**_.
